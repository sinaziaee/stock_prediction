{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import utils as utils\n",
    "from datetime import datetime as dt\n",
    "from newspaper import Article\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.investing.com/equities/trending-stocks'\n",
    "BASE_URL = 'https://www.investing.com'\n",
    "request = requests.get(url).text\n",
    "raw_text = BeautifulSoup(request, 'html.parser')\n",
    "trending_stocks = raw_text.find('div', {'id': 'trendingInnerContent'})\n",
    "stock_elements = trending_stocks.find_all('td', class_='left bold plusIconTd elp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stock_info(stock_elements):\n",
    "    stock_dict = {'company': [], 'stock': [], 'link': []}\n",
    "    for element in stock_elements:\n",
    "        company_name = element.find('a')['title']\n",
    "        company_name = company_name.replace('\\xa0', ' ')\n",
    "        stock_name = element.find('a').text\n",
    "        link = element.find('a')['href']\n",
    "        full_link = f'{BASE_URL}{link}'\n",
    "        stock_dict['company'].append(company_name)\n",
    "        stock_dict['stock'].append(stock_name)\n",
    "        stock_dict['link'].append(full_link)\n",
    "    df = pd.DataFrame(stock_dict)\n",
    "    utils.create_path('../datasets')\n",
    "    df.to_csv('../datasets/stocks.csv', index=False)\n",
    "    return stock_dict, df\n",
    "stock_dict, df = extract_stock_info(stock_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list_path = utils.create_path('../datasets/news_links')\n",
    "def extract_news_links(stock_dict, news_list_path, max_num_pages=1):\n",
    "    for inx, (stock_name, link) in enumerate(zip(stock_dict['stock'], stock_dict['link'])):\n",
    "        try:\n",
    "            full_link = f'{link}-news'\n",
    "            for page in range(1, max_num_pages + 1):\n",
    "                full_link = f'{link}-news/{page}'\n",
    "                request = requests.get(full_link).text\n",
    "                bs4 = BeautifulSoup(request, 'html.parser')\n",
    "                news_table = bs4.find('ul', {'data-test': 'news-list'})\n",
    "                news_list = news_table.find_all('article', {'data-test': 'article-item'})\n",
    "                with open(f'{news_list_path}/{stock_name}.txt', 'w') as file:\n",
    "                    for news_data in news_list:\n",
    "                        if str(news_data).find('mt-2.5') == -1:\n",
    "                            news_link = news_data.findAll('a')[1]['href']\n",
    "                            full_link = f'{BASE_URL}{news_link}'\n",
    "                            file.write(f'{full_link}\\n')\n",
    "        except Exception as e:\n",
    "            print(f'Error for stock {stock_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_of_links(news_list_path):\n",
    "    news_dict = {}\n",
    "    for file_name in os.listdir(news_list_path):\n",
    "        with open(f'{news_list_path}/{file_name}', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            lines = list(set(lines))\n",
    "        stock_name = file_name.replace('.txt', '')\n",
    "        for line in lines:\n",
    "            if stock_name in news_dict:\n",
    "                news_dict[stock_name].append(line.replace('\\n', ''))\n",
    "            else:\n",
    "                news_dict[stock_name] = [line.replace('\\n', '')]\n",
    "    return news_dict\n",
    "news_dict = create_dict_of_links(news_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news(news_dict):\n",
    "    df = pd.DataFrame(columns=['stock', 'title', 'text', 'date', 'time', 'am_pm'])\n",
    "    stock_list = []\n",
    "    title_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "    am_pm_list = []\n",
    "    text_list = []\n",
    "    for inx, stock_name in enumerate(news_dict):\n",
    "        if inx > 5:\n",
    "            break\n",
    "        for link in news_dict[stock_name]:\n",
    "            stock_list.append(stock_name)\n",
    "            request = requests.get(link).text\n",
    "            bs4 = BeautifulSoup(request, 'html.parser')\n",
    "            # parsing the title of the article\n",
    "            try:\n",
    "                header = bs4.find('h1', {'class': 'articleHeader'}).text\n",
    "                title_list.append(header)\n",
    "            except Exception as e:\n",
    "                title_list.append(None)\n",
    "                print(f'Error in parsing \"\"Title(header)\"\" in stock: {stock_name} is: {e}')\n",
    "            # parsing the date and time of the article\n",
    "            try:\n",
    "                datetime = bs4.findAll('div', {'class': 'contentSectionDetails'})[1].find('span').text\n",
    "                datetime = datetime.replace('Published ', '')[:-3]\n",
    "                datetime = dt.strptime(datetime, '%b %d, %Y %I:%M%p')\n",
    "                time = datetime.strftime('%H:%M')\n",
    "                date = datetime.strftime('%Y-%m-%d')\n",
    "                am_pm = datetime.strftime('%p')\n",
    "                date_list.append(date)\n",
    "                time_list.append(time)\n",
    "                am_pm_list.append(am_pm)\n",
    "            except Exception as e:\n",
    "                date_list.append(None)\n",
    "                time_list.append(None)\n",
    "                am_pm_list.append(None)\n",
    "                print(f'Error in parsing \"\"datetime\"\" in stock: {stock_name} is: {e}')\n",
    "            # parsing the body of the article\n",
    "            article = Article(link)\n",
    "            article.download()\n",
    "            try:\n",
    "                article.parse() \n",
    "                text = article.text\n",
    "                if text.startswith('Published'):\n",
    "                    index = text.find('\\n') + 2\n",
    "                    text = text[index:]\n",
    "                text_list.append(text)  \n",
    "            except Exception as e:\n",
    "                text_list.append(None)\n",
    "                print(f'Error in parsing \"\"article body\"\" in stock: {stock_name} is: {e}')\n",
    "    df['stock'], df['title'], df['text'] = stock_list, title_list, text_list\n",
    "    df['date'], df['time'], df['am_pm'] = date_list, time_list, am_pm_list   \n",
    "    return df\n",
    "df = extract_news(news_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../datasets/news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     © Reuters Adobe (ADBE) Brings Conversational A...\n",
       "1     © Reuters. Figurines with computers and smartp...\n",
       "2     © Reuters. FILE PHOTO: Volunteers watch for vo...\n",
       "3     © Reuters. FILE PHOTO: European Union flags fl...\n",
       "4     © Reuters. FILE PHOTO: People walk behind a lo...\n",
       "5     © Reuters. Amazon Withdraws From iRobot Deal A...\n",
       "6     © Reuters.\\n\\nCRM -0.30% Add to/Remove from Wa...\n",
       "7     © Reuters.\\n\\nADBE +2.95% Add to/Remove from W...\n",
       "8     © Reuters. The exterior of the Warner Bros. Di...\n",
       "9     © Reuters. Alibaba (BABA) cut at Macquarie as ...\n",
       "10    © Reuters. FILE PHOTO: A Squishmallow depictin...\n",
       "11    2/2 © Reuters. FILE PHOTO: A keyboard and a sh...\n",
       "12    © Reuters. FILE PHOTO: A passerby walks past a...\n",
       "13    © Reuters\\n\\nBABA -0.21% Add to/Remove from Wa...\n",
       "14    © Reuters.\\n\\nUS500 +0.03% Add to/Remove from ...\n",
       "15    © Reuters. FILE PHOTO: A smartphone with a dis...\n",
       "16                            Please try another search\n",
       "17    © Reuters. FILE PHOTO: A self-driving GM Bolt ...\n",
       "18    © Reuters\\n\\nROKU +1.88% Add to/Remove from Wa...\n",
       "19    © Reuters. Alphabet (GOOGL) target lifted as s...\n",
       "20    © Reuters.\\n\\nGOOGL -0.09% Add to/Remove from ...\n",
       "21    © Reuters. FILE PHOTO: Figurines with computer...\n",
       "22    5/5 © Reuters. Former U.S. President and Repub...\n",
       "23    © Reuters. FILE PHOTO: A NVIDIA logo is shown ...\n",
       "24    © Reuters. FILE PHOTO: A smartphone with a dis...\n",
       "25    © Reuters. FILE PHOTO: NVIDIA HGX AI Supercomp...\n",
       "26    © Reuters\\n\\nROKU +1.88% Add to/Remove from Wa...\n",
       "27    TaskUs (TASK) Collaborates with AWS over TaskG...\n",
       "28    © Reuters. FILE PHOTO: A Verizon logo is seen ...\n",
       "29    © Reuters. CMB starts Microsoft (MSFT) and Ama...\n",
       "30    © Reuters. Microsoft and OpenAI-backed robotic...\n",
       "31    © Reuters. FILE PHOTO: Traders work on the flo...\n",
       "32    © Reuters. FILE PHOTO: A NVIDIA logo is shown ...\n",
       "33    © Reuters. FILE PHOTO: NVIDIA HGX AI Supercomp...\n",
       "34    © Reuters\\n\\nUS500 +0.03% Add to/Remove from W...\n",
       "35    © Reuters. Traders work on the floor at the Ne...\n",
       "36    © Reuters.\\n\\nNVDA +0.36% Add to/Remove from W...\n",
       "37    © Reuters\\n\\nNVDA +0.36% Add to/Remove from Wa...\n",
       "38    © Reuters. A smartphone with a displayed Arm l...\n",
       "39    © Reuters. Semiconductor chips are seen on a p...\n",
       "40    © Reuters. FILE PHOTO: NVIDIA HGX AI Supercomp...\n",
       "41    Why Is AMD (AMD) Stock Soaring Today\\n\\nNVDA +...\n",
       "42    © Reuters\\n\\nUS500 +0.03% Add to/Remove from W...\n",
       "43    © Reuters. FILE PHOTO: Berkshire Hathaway Vice...\n",
       "44    © Reuters Short sellers take a $2.9 billion hi...\n",
       "45    © Reuters. FILE PHOTO: The Apple Inc logo is s...\n",
       "46    © Reuters. FILE PHOTO: A customer uses Apple's...\n",
       "47    © Reuters. Traders work on the floor at the Ne...\n",
       "48    © Reuters. Magnificent 7 helps push mutual fun...\n",
       "49    © Reuters. Men walk looking at their smartphon...\n",
       "50    © Reuters. FILE PHOTO: Traders work on the flo...\n",
       "51    © Reuters. FILE PHOTO: A NVIDIA logo is shown ...\n",
       "52    © Reuters. FILE PHOTO: Berkshire Hathaway Chai...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
